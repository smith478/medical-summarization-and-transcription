{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71435cb1-e385-49fd-abdc-6d617c6c49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "import scipy.io.wavfile as wavfile\n",
    "from scipy.signal import resample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6414aa6-8d3a-468b-b6be-230bef921dd3",
   "metadata": {},
   "source": [
    "## Load audio file and get the correct sample rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc0db2-f90a-4596-aec1-4c107ae905db",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FILENAME = 'sample_patient_history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb5c25-079b-405b-84ff-90f15ca6e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file and get the sample rate and audio data\n",
    "sample_rate, audio_data = wavfile.read(f'../sample_audio/{BASE_FILENAME}.wav')\n",
    "\n",
    "# Ensure audio data is within range for int16\n",
    "audio_data = np.clip(audio_data, -32768, 32767).astype(np.int16)\n",
    "\n",
    "# audio_data is now a NumPy array containing the audio samples\n",
    "# sample_rate contains the sample rate of the audio file\n",
    "\n",
    "# You can perform operations on the audio data as needed\n",
    "# For example, you can print the length of the audio in seconds:\n",
    "audio_length_seconds = len(audio_data) / sample_rate\n",
    "print(f\"Audio Length: {audio_length_seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98f09c-a778-43fe-bc4f-832097e5a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the audio in the notebook\n",
    "Audio(audio_data.T, rate=sample_rate)\n",
    "# Audio(audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1cad3-9a7a-49b9-a2ed-63293ea51799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target sample rate (16,000 Hz)\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# Calculate the resampling factor\n",
    "resampling_factor = target_sample_rate / sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f57060-dc61-4053-9b2a-8f1adb53952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the audio data\n",
    "resampled_audio_data = resample(audio_data, int(len(audio_data) * resampling_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f5f09-2ff9-4d1b-8911-c773c8cdcc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the resampled audio to a new WAV file\n",
    "wavfile.write(f'../sample_audio/{BASE_FILENAME}_resample.wav', target_sample_rate, np.int16(resampled_audio_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481de9a-0d63-4bd5-9015-79d4651548f2",
   "metadata": {},
   "source": [
    "## Run Whisper transcription model on audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee65e3-b6df-4c4f-968d-6d5144885faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3514981-d336-46b3-b0e6-8fefe2bd3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'openai/whisper-large-v3'\n",
    "# MODEL_ID = 'openai/whisper-large-v2'\n",
    "# MODEL_ID = 'openai/whisper-tiny'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6c5d6-1eed-4b81-90c2-c4cfa0103006",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(MODEL_ID)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "model.config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5a99a-63fd-40f3-bf84-8d1dfe976413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file and get the sample rate and audio data\n",
    "# sample_rate, audio_data = wavfile.read(f'../samples/{BASE_FILENAME}_resample.wav')\n",
    "# sample_rate, audio_data = wavfile.read(f'../samples/output_audio_original.wav')\n",
    "sample_rate, audio_data = wavfile.read(f'../sample_audio/output_audio_target_sample_rate.wav')\n",
    "\n",
    "# audio_data is now a NumPy array containing the audio samples\n",
    "# sample_rate contains the sample rate of the audio file\n",
    "\n",
    "# You can perform operations on the audio data as needed\n",
    "# For example, you can print the length of the audio in seconds:\n",
    "audio_length_seconds = len(audio_data) / sample_rate\n",
    "print(f\"Audio Length: {audio_length_seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ecb88-4226-4efb-95f0-7004047d5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the audio in the notebook\n",
    "Audio(audio_data.T, rate=sample_rate)\n",
    "# Audio(audio_data, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebffd26-2bfa-472e-afcb-b5ebb783d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_features = processor(audio_data, sampling_rate=sample_rate, return_tensors=\"pt\").input_features \n",
    "input_features = processor(audio_data.T, sampling_rate=sample_rate, return_tensors=\"pt\").input_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5ae5c-153f-4212-999c-12c7ca295a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate token ids\n",
    "# predicted_ids = model.generate(input_features)\n",
    "# # decode token ids to text\n",
    "# # transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n",
    "# transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0da57-1a61-4274-b60b-0c9cfac36130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples per chunk\n",
    "samples_per_chunk = int(sample_rate * 30)\n",
    "\n",
    "# Split the audio data into chunks\n",
    "chunks = np.array_split(audio_data, np.ceil(len(audio_data) / samples_per_chunk))\n",
    "\n",
    "transcriptions = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    input_features = processor(chunk.T, sampling_rate=sample_rate, return_tensors=\"pt\").input_features \n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    transcriptions.append(transcription[0])\n",
    "\n",
    "# Join the transcriptions together\n",
    "transcription = ' '.join(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb49286e-8e87-4680-85e8-8d30b21d5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7346fb5d-cbc4-41cc-9094-ef31931c414c",
   "metadata": {},
   "source": [
    "## Run Nvidia transcription models on audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6838e12-6f1e-4005-b1f8-afce88523a1d",
   "metadata": {},
   "source": [
    "#### Parakeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4796ba-b480-4f6f-a091-82feeee65d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-1.1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bee7f-a8a1-496f-b4db-e54622ce02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model.transcribe(['../sample_audio/sample_patient_history_resample.wav'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d379f-9340-4a2d-a4cf-2b6b181e27b2",
   "metadata": {},
   "source": [
    "#### Canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0a1aa-6a26-4df0-b783-f6c46036aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/NVIDIA/NeMo.git@r1.23.0#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4e311-f26a-405b-8d58-f56f70ff2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import EncDecMultiTaskModel\n",
    "\n",
    "# load model\n",
    "canary_model = EncDecMultiTaskModel.from_pretrained('nvidia/canary-1b')\n",
    "\n",
    "# update dcode params\n",
    "decode_cfg = canary_model.cfg.decoding\n",
    "decode_cfg.beam.beam_size = 1\n",
    "canary_model.change_decoding_strategy(decode_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fd1ab-7a12-436e-b59e-359977175328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
