{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917032e2-1d2b-461b-92fb-2808cdeba85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e86772-8786-411c-ba38-abf4e16e4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bda51-207d-4922-ac63-5ec1b6578f9a",
   "metadata": {},
   "source": [
    "## Load the pubmed summarization dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f8b39-60f4-4603-8c21-85b8ee1c315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ccdv/pubmed-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65e864-5daa-4d74-b90a-b9c1c55151ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9ec6f-abd4-4221-aa18-7ecf2d581791",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset['train'][1]\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample['article'][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"abstract\"])}):')\n",
    "print(sample[\"abstract\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae422f36-c0f9-4a65-8b9f-bc4056bbd18b",
   "metadata": {},
   "source": [
    "## Load the pretrained models\n",
    "\n",
    "We could also load these models using `AutoModel` and `AutoTokenizer` to find the maximum input length. It would look something like:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = 'Falconsai/medical_summarization'\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Get the maximum input size\n",
    "max_input_size = model.config.max_position_embeddings\n",
    "print(f\"The maximum input size of the model is: {max_input_size}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c863c-5cde-4634-80fa-fd2604b90b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_falconsai = pipeline(\"summarization\", model=\"Falconsai/medical_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68830ba-6b53-4711-bc27-6a6f12864f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_longt5 = pipeline(\"summarization\", model=\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812f694-273f-4a71-be5e-e1c1b57c4522",
   "metadata": {},
   "source": [
    "## Sample text comparison\n",
    "\n",
    "Here we'll take the first 3000 characters in order to have the same input for each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b7d49-93f3-4f1a-af5b-87334097d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = sample['article'][:5000]\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e7564-239b-441c-b3e4-720d90cf12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b41c01-e05e-4fad-9ba0-4b8bce79e852",
   "metadata": {},
   "source": [
    "## Evaluate pretrained models\n",
    "\n",
    "As a baseline we will use the first three sentences as the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673de4a-489e-4d86-8255-388596ace7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43776f97-8fba-4e86-8cb2-ba27fbee7357",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f784993-2b79-4bc0-ba1d-7820ebb611ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685e7cc-51b6-4d0c-8c13-131353e758c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"baseline\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb06bf-5b4c-4788-8628-0ba971ab5274",
   "metadata": {},
   "source": [
    "#### Falconsai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878424d5-3fab-44eb-9595-023eecf77078",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = summarizer_falconsai(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4789fe-ab35-44ad-903c-904d27c2e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a1bb1-0f16-485a-b7d8-ffbdc57c2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"falconsai\"] = \"\\n\".join(sent_tokenize(output[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f19a9-208a-439c-b337-c46199121d92",
   "metadata": {},
   "source": [
    "#### long t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367ab04-c219-49f7-ba96-e37ef3a4167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = summarizer_longt5(sample_text, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dba02c-883b-4bf2-a80e-8a157eca58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146969d-06b0-47be-981d-0aaf8e38fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"longt5\"] = \"\\n\".join(sent_tokenize(output[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21a7f2-236b-4936-8824-96021725b4e2",
   "metadata": {},
   "source": [
    "## Comparing different summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64535522-689a-45cc-88e9-1c064a1e0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "print(sample['abstract'])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68633112-5c7a-4cbd-a985-c981826155b5",
   "metadata": {},
   "source": [
    "## Evaluate quality of summaries\n",
    "\n",
    "We will use the ROUGE metric to compare the quality of different summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e1113-3d45-4fbe-91d6-21d6218c8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde492ea-f17d-4740-bde2-660f757f46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = sample['abstract']\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    records.append(rouge_dict)\n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a6282-34d7-4b02-9e59-14c2dce345ca",
   "metadata": {},
   "source": [
    "## Evaluating the models on the validation dataset\n",
    "\n",
    "To keep the evaluation time reasonable we will choose a subset of 200 examples from the test dataset to compare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c8f7c-30da-4e5e-83bc-86879171c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef47fe9-dcf4-4af0-b85c-afebad34f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c030b-6488-40b2-9937-7e883a65474b",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57411b7a-1050-4798-ba22-ed874c3b417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric,\n",
    "                               column_text='article',\n",
    "                               column_summary='abstract'):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "    metric.add_batch(predictions=summaries,\n",
    "                     references=dataset[column_summary])\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109e512-39db-4ca4-a70b-74619f20d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fa3d7-4b89-4f7c-93af-1d553db396cc",
   "metadata": {},
   "source": [
    "#### falconsai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded4361-5569-4960-a9ec-b38b6526b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0ddb1-0717-4159-b97b-4a82290e875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abfd7a-9771-4a27-a888-1880a32a800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(list_of_elements, batch_size):\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i: i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34ef45-b7fe-43de-866e-2947d7bd473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries(dataset, metric, model, tokenizer,\n",
    "                      batch_size=16, device=device,\n",
    "                      column_text=\"article\", column_summary=\"abstract\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for i, (article_batch, target_batch) in enumerate(zip(article_batches, target_batches)):\n",
    "        if i % 10 == 0:\n",
    "            print(f'Running batch {i + 1} of {len(article_batches)}')\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=2048, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                   length_penalty=0.8, num_beams=8, max_length=256)\n",
    "\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_space=True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ede6dd-067f-495a-b8d9-767b1281af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199e140-f289-40ff-9e1b-bad5645a169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"Falconsai/medical_summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13a28a-4cce-4ca5-9288-77565004312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e79144-9fe7-4570-96af-71ef360f29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5477ff-d407-40df-8dcc-cf25e1f00c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate_summaries(test_sampled, rouge_metric, model, tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b23b29-0bec-4db6-adde-2bc6ce636b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index=[\"falconsai\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca18a3-4b12-4562-9d70-03466729a239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
